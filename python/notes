#TODO - test all this locally.  test weightsgen, then then proofreader.

'''
take substrate text
convert to padded list of char arrays of length seglen

build numpy bool matrices x and y like for training

for each segment and next_char 1-hot vector arrays and vector,
plug the vector array into LSTM, get probaility of next_char as listed in output/predictions

add this tuple to a list of results:  char and probability.

'''

# okay now find something more substantial to train on . t rain overnihgt
# wiki text dump

#use percentile!
'''

import scipy.stats as stats

stats.percentileofscore([0,1,2,3,4,5], 0)

x = [6,5,4,3,2,1,2,3,4,5,0,7,8,6,7,5,6,4]
>>> stats.percentileofscore(x, 0)
5.5555555555555554
>>> stats.percentileofscore(x, 8)
100.0
>>> stats.percentileofscore(x, -1)
0.0

why isn't percentile of lowest score 0?

how else to improve?

'''





    # html = h.getColoredHtmlText(output)
    #
    # print(html)
    #
    # with open("Output.html", "w") as text_file:
    #     text_file.write(html)

the chars i decided to keep:


{0: '\n', 1: ' ', 2: '!', 3: '"', 4: '$', 5: "'", 6: ',', 7: '-', 8: '.', 9: '0', 10: '1', 11: '2', 12: '3', 13: '4', 14: '5', 15: '6', 16: '7', 17: '8', 18: ':', 19: ';', 20: '?', 21: '[', 22: ']', 23: '_', 24: 'a', 25: 'b', 26: 'c', 27: 'd', 28: 'e', 29: 'f', 30: 'g', 31: 'h', 32: 'i', 33: 'j', 34: 'k', 35: 'l', 36: 'm', 37: 'n', 38: 'o', 39: 'p', 40: 'q', 41: 'r', 42: 's', 43: 't', 44: 'u', 45: 'v', 46: 'w', 47: 'x', 48: 'y', 49: 'z', 50: 'ᚙ', 51: '9', 52: '&', 53: '(', 54: ')', 55: '*', 56: '°', 57: 'æ', 58: 'é', 59: '+', 60: '/', 61: 'ë', 62: 'ö', 63: '`', 66: '=', 71: 'ü', 72: '{', 73: '|', 74: '}', 75: '·', 77: 'à', 78: 'á', 79: 'â', 80: 'è', 82: '£', 84: 'ä', 85: 'ê', 86: 'ô', 89: '\xa0', 90: 'ú', 91: 'í', 92: 'ç', 95: 'ñ', 96: 'û', 100: '—', 101: '‘', 102: '’', 103: '“', 104: '”', 106: '\x97', 107: '§', 109: '#', 112: 'œ', 119: 'α', 124: 'τ', 126: 'ι', 128: 'ν', 129: 'ο', 132: 'σ', 161: 'ε', 210: '‐', 303: 'ᚊ'}
>>>


trash:



#
# import h_keras
#
# model = h_keras.baseline_model(len(chars), seglen)
#
# model.load_weights(h.getWeightsFile())
#
# output = []  # char_actual, p, pmin, pmax, char_pmax
#
# for i, segment_one_hot_arrays in enumerate(x):
#     x_pred = segment_one_hot_arrays
#     shape = x_pred.shape
#     x_pred = x_pred.reshape((1, shape[0], shape[1]))
#     # print(x_pred)
#     preds = model.predict(x_pred, verbose=0)[0]
#     next_index = np.argmax(y[i])
#     next_index_probability = preds[next_index]
#     pmin = np.amin(preds)
#     pmax_index = np.argmax(preds)
#     pmax = preds[pmax_index]
#     output.append((m_index_char[next_index], next_index_probability, pmin, pmax, m_index_char[pmax_index]))
# #
# for i, segment_one_hot_arrays in enumerate(x):
#     x_pred = segment_one_hot_arrays
#     shape = x_pred.shape
#     x_pred = x_pred.reshape((1, shape[0], shape[1]))
#     # print(x_pred)
#     preds = model.predict(x_pred, verbose=0)[0]
#     next_index = np.argmax(y[i])
#     next_index_probability = preds[next_index]
#     pmin = np.amin(preds)
#     pmax_index = np.argmax(preds)
#     pmax = preds[pmax_index]
#     output.append((m_index_char[next_index], next_index_probability, pmin, pmax, m_index_char[pmax_index]))


#
# from keras.preprocessing import sequence
#
# segments = []
# next_indices = []
# for i in range(0, len(indices)):
#     segments.append(indices[min(i - seglen, 0): i])
#     # print(segments[i])
#     next_indices.append(indices[i])
#
# print('nb sequences:', len(segments))
#
# # xIndices = sequence.pad_sequences(segments, seglen)
#
# # sequence.pad_sequences([['c', 'f', 'g']], 10)   # doesn't work
#
# print('Vectorization...')
# x = np.zeros((len(segments), seglen, len(chars)), dtype=np.bool)
# y = np.zeros((len(segments), len(chars)), dtype=np.bool)
# for i, segment in enumerate(segments):
#     padded = sequence.pad_sequences([segment], seglen)[0]
#     # print(padded)
#     for t, char_index in enumerate(padded):
#         x[i, t, char_index] = 1
#     # print(x[i])
#     y[i, next_indices[i]] = 1

# okay x and y are ready
